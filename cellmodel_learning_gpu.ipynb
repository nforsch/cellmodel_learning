{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:35:42.223234Z",
     "start_time": "2022-09-21T09:35:02.630607Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import pydot\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, RepeatedKFold, cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "plt.rcParams['font.family'] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T15:10:54.992928Z",
     "start_time": "2022-08-02T15:10:50.828996Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'tf.test.is_built_with_cuda(): {tf.test.is_built_with_cuda()}');\n",
    "\n",
    "if tf.__version__[0] == '1':\n",
    "    print(f'tf.config.experimental_list_devices(): {tf.config.experimental_list_devices()}');\n",
    "else:\n",
    "    print(f'tf.config.list_physical_devices(\"GPU\"): {tf.config.list_physical_devices(\"GPU\")}');\n",
    "    \n",
    "print(f'tf.test.gpu_device_name(): {tf.test.gpu_device_name()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:35:42.229843Z",
     "start_time": "2022-09-21T09:35:42.224958Z"
    }
   },
   "outputs": [],
   "source": [
    "# randst = np.random.randint(0,100)\n",
    "randst = 94\n",
    "np.random.seed(randst)\n",
    "print(randst)\n",
    "\n",
    "verbosity = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:35:43.396700Z",
     "start_time": "2022-09-21T09:35:43.390462Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_scores(y_test, y_hat):\n",
    "    mae = mean_absolute_error(y_test, y_hat)\n",
    "    mre = mean_absolute_percentage_error(y_test, y_hat)\n",
    "    r2 = r2_score(y_test, y_hat)\n",
    "    metrics = [mae, mre, r2]\n",
    "    mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "    mre_per = mean_absolute_percentage_error(y_test, y_hat, multioutput=\"raw_values\")\n",
    "    r2_per = r2_score(y_test, y_hat, multioutput='raw_values')\n",
    "    metrics_per = [mae_per, mre_per, r2_per]\n",
    "    return metrics, metrics_per\n",
    "\n",
    "def print_my_results(results_mae, results_mae_per, results_r2, results_r2_per, results_mre=None, results_mre_per=None):\n",
    "    \n",
    "    print('MAE:')\n",
    "    print('\\t%.3f (%.3f) overall' % (np.mean(results_mae), np.std(results_mae)))\n",
    "    for mae,std,idx in zip(np.mean(results_mae_per, axis=0), np.std(results_mae_per, axis=0), idx_params):\n",
    "        print('\\t%.3f (%.3f) %s' % (mae, std, labels[idx]) )\n",
    "    if results_mre is not None:\n",
    "        print('MRE:')\n",
    "        print('\\t%.3f (%.3f) overall' % (np.mean(results_mre), np.std(results_mre)))\n",
    "        for mre,std,idx in zip(np.mean(results_mre_per, axis=0), np.std(results_mre_per, axis=0), idx_params):\n",
    "            print('\\t%.3f (%.3f) %s' % (mre, std, labels[idx]) )\n",
    "    print('R2:')\n",
    "    print('\\t%.3f (%.3f) overall' %(np.mean(results_r2), np.std(results_r2)))\n",
    "    for r2,std,idx in zip(np.mean(results_r2_per, axis=0), np.std(results_r2_per, axis=0), idx_params):\n",
    "        print('\\t%.3f (%.3f) %s' % (r2, std, labels[idx]) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:35:50.096330Z",
     "start_time": "2022-09-21T09:35:50.094056Z"
    }
   },
   "outputs": [],
   "source": [
    "scalar_on = 1\n",
    "cdt_on = 0\n",
    "drug_train_on = 1\n",
    "downsample_on = 1\n",
    "N_train = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and test case indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:39:07.245753Z",
     "start_time": "2022-09-21T09:39:06.804367Z"
    }
   },
   "outputs": [],
   "source": [
    "name = \"n4130\"\n",
    "here = Path.cwd()\n",
    "data_path = Path(here.joinpath(f\"data/data_control_{name}.h5\"))\n",
    "\n",
    "with h5py.File(data_path, 'r') as f:\n",
    "    Datasetnames=f.keys()\n",
    "    print(*list(Datasetnames), sep = \"\\n\")\n",
    "    trace = f['trace'][:,:200,:] # select time 0-200\n",
    "    t = f['time'][...]\n",
    "    adj_factors = f['adjustment_factors'][...]\n",
    "    cost_terms = f['cost_terms'][...]\n",
    "    \n",
    "if trace.shape[0] != adj_factors.shape[0]:\n",
    "    print('Number of samples do not match for trace and adj_factors!')\n",
    "\n",
    "N_def = trace.shape[0]\n",
    "print(\"Number of traces (control):\", N_def)\n",
    "labels = [\"g_Kr\",\"g_CaL\",\"lambda_B\",\"g_NaCa\",\"g_K1\",\"J_SERCA_bar\",\"lambda_diff\",\"lambda_RyR\",\"g_bCa\",\"g_Na\",\"g_NaL\"]\n",
    "\n",
    "# Separate test cases (n=50) from all cases\n",
    "idx_all = list(np.arange(0,trace.shape[0]))\n",
    "idx_test = list(np.loadtxt(here.joinpath(\"data/idx_key_p11_s100_n5000_ns50.txt\"), dtype=int))\n",
    "idx_train = list(set(idx_all) - set(idx_test))\n",
    "\n",
    "if drug_train_on:\n",
    "    data_path = Path(here.joinpath(f\"data/data_drug-combo_blockX_{name}.h5\"))\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        trace = np.vstack((trace, f['trace'][:,:200,:])) # select time 0-200\n",
    "        adj_factors = np.vstack((adj_factors, f['adjustment_factors'][...]))\n",
    "        cost_terms = np.vstack((cost_terms, f['cost_terms'][...]))\n",
    "    idx_all = list(np.arange(0,trace.shape[0]))\n",
    "    a = np.loadtxt(here.joinpath(\"data/idx_key_p11_s100_n5000_ns50.txt\"), dtype=int)\n",
    "    idx_test = list(a)\n",
    "    idx_exclude = list(np.concatenate((a, a+N_def))) # remove test and modified test from idx_train\n",
    "    idx_train = list(set(idx_all) - set(idx_exclude))\n",
    "    N_def = trace.shape[0]\n",
    "    print(\"Number of traces (all):\", N_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition data\n",
    "Test cases are pre-selected for out-of-sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:39:08.748632Z",
     "start_time": "2022-09-21T09:39:08.722985Z"
    }
   },
   "outputs": [],
   "source": [
    "if not cdt_on:\n",
    "    print(f\"{cdt_on=}\")\n",
    "    idx_params = [0,1,4,9,10]\n",
    "    trace_train = trace[idx_train,:,:]\n",
    "    trace_test = trace[idx_test,:,:]\n",
    "    af_train = adj_factors[idx_train,:][:,idx_params]\n",
    "    af_test = adj_factors[idx_test,:][:,idx_params]\n",
    "\n",
    "    if downsample_on:\n",
    "        print(f\"{downsample_on=}\")\n",
    "        idx_down = np.random.choice(af_train.shape[0], N_train, replace=False)\n",
    "        trace_train = trace_train[idx_down,:,:]\n",
    "        af_train = af_train[idx_down,:]\n",
    "        \n",
    "elif cdt_on:\n",
    "    print(f\"{cdt_on=}\")\n",
    "    import transportBasedTransforms.cdt as CDT\n",
    "    trace_cdt = np.zeros_like(trace)\n",
    "    N=200\n",
    "    I0= (1.0/N)*np.ones(N)\n",
    "    cdt=CDT.CDT(template=I0)\n",
    "\n",
    "    for i in range(trace.shape[0]):\n",
    "        for j in range(trace.shape[2]):\n",
    "            trace_cdt[i,:,j] = cdt.transform(trace[i,:,j])\n",
    "\n",
    "    idx_params = [0,1,4,9,10]\n",
    "    trace_train = trace_cdt[idx_train,:,:]\n",
    "    trace_test = trace_cdt[idx_test,:,:]\n",
    "    af_train = adj_factors[idx_train,:][:,idx_params]\n",
    "    af_test = adj_factors[idx_test,:][:,idx_params]\n",
    "    \n",
    "    if downsample_on:\n",
    "        print(f\"{downsample_on=}\")\n",
    "        idx_down = np.random.choice(af_train.shape[0], N_train, replace=False)\n",
    "        trace_train = trace_train[idx_down,:,:]\n",
    "        af_train = af_train[idx_down,:]\n",
    "        \n",
    "print(\"Number of training cases:\", af_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug effect target data\n",
    "Import as 4 datasets: control, drug-Kr, drug-CaL, drug-combo\n",
    "\n",
    "No partitioning needed since training with control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:21:45.265640Z",
     "start_time": "2022-07-04T11:21:45.133980Z"
    }
   },
   "outputs": [],
   "source": [
    "trace_drug = []\n",
    "af_drug = []\n",
    "\n",
    "block = \"block20\"\n",
    "names_data = [\"control\", f\"drug-Kr_{block}\", f\"drug-CaL_{block}\", f\"drug-combo_{block}\"]\n",
    "\n",
    "here = Path.cwd()\n",
    "\n",
    "for name in names_data:\n",
    "    data_path = Path(here.joinpath(f\"data/data_{name}_n50.h5\"))\n",
    "    with h5py.File(data_path, \"r\") as f:\n",
    "        trace_drug.append(f[\"trace\"][:,:200,:])\n",
    "        af_drug.append(f[\"adjustment_factors\"][:,:])\n",
    "\n",
    "trace_drug = np.array(trace_drug)\n",
    "af_drug = np.array(af_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:21:45.583383Z",
     "start_time": "2022-07-04T11:21:45.579751Z"
    }
   },
   "outputs": [],
   "source": [
    "if not cdt_on:\n",
    "    trace_drug_test = np.copy(trace_drug)\n",
    "    af_drug_test = af_drug[:,:,idx_params]\n",
    "\n",
    "elif cdt_on:\n",
    "    trace_drug_test = np.zeros_like(trace_drug)\n",
    "\n",
    "    for k in range(trace_drug_test.shape[0]):\n",
    "        for i in range(trace_drug_test.shape[1]):\n",
    "            for j in range(trace_drug_test.shape[3]):\n",
    "                trace_drug_test[k,i,:,j] = cdt.transform(trace_drug[k,i,:,j])\n",
    "\n",
    "    af_drug_test = af_drug[:,:,idx_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug effect target data - mps-prelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:39:13.442121Z",
     "start_time": "2022-09-21T09:39:13.433402Z"
    }
   },
   "outputs": [],
   "source": [
    "here = Path.cwd()\n",
    "\n",
    "data_path = Path(here.joinpath(\"data/data_drug-prelim2_n9.h5\"))\n",
    "with h5py.File(data_path, \"r\") as f:\n",
    "    trace_drug = np.zeros((9,200,2))\n",
    "    print(list(f.keys()))\n",
    "    trace_drug[:,:,0] = f[\"V\"]\n",
    "    trace_drug[:,:,1] = f[\"Ca\"]\n",
    "    \n",
    "trace_test = trace_drug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning for kNN, RF, SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T15:44:38.168214Z",
     "start_time": "2022-06-01T15:44:38.138170Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "if scalar_on:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T15:47:02.775463Z",
     "start_time": "2022-06-01T15:44:55.584054Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_grid = {\"n_neighbors\": np.arange(1, 20),\n",
    "            \"weights\": ['uniform', 'distance'],\n",
    "            \"p\": [1,2],\n",
    "              }\n",
    "\n",
    "knn_base = KNeighborsRegressor()\n",
    "knn_gscv = GridSearchCV(estimator = knn_base, param_grid = knn_grid,\n",
    "                        cv=5, verbose=2,\n",
    "                        n_jobs=-1)\n",
    "knn_gscv.fit(X, y)\n",
    "\n",
    "print(knn_gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T12:14:39.727171Z",
     "start_time": "2022-01-06T12:14:39.698549Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T13:46:23.191697Z",
     "start_time": "2022-01-06T12:56:13.369729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of trees in Random Forest\n",
    "rf_n_estimators = [int(x) for x in np.linspace(200, 1000, 5)]\n",
    "rf_n_estimators.append(1500)\n",
    "rf_n_estimators.append(2000)\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "rf_max_depth = [int(x) for x in np.linspace(5, 55, 6)]\n",
    "rf_max_depth.append(None)\n",
    "\n",
    "# Number of features to consider at every split\n",
    "# rf_max_features = ['auto', 'sqrt', 'log2']\n",
    "rf_max_features = ['log2']\n",
    "\n",
    "# Criterion to split on\n",
    "rf_criterion = ['absolute_error']\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "rf_min_samples_split = [int(x) for x in np.linspace(2, 10, 9)]\n",
    "\n",
    "# Minimum decrease in impurity required for split to happen\n",
    "rf_min_impurity_decrease = [0.0, 0.05, 0.1]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "rf_bootstrap = [True, False]\n",
    "\n",
    "# Create the grid\n",
    "rf_grid = {'n_estimators': rf_n_estimators,\n",
    "               'max_depth': rf_max_depth,\n",
    "               'max_features': rf_max_features,\n",
    "               'criterion': rf_criterion,\n",
    "               'min_samples_split': rf_min_samples_split,\n",
    "               'min_impurity_decrease': rf_min_impurity_decrease,\n",
    "               'bootstrap': rf_bootstrap}\n",
    "\n",
    "rf_base = RandomForestRegressor()\n",
    "\n",
    "# Create the random search Random Forest\n",
    "rf_gscv = RandomizedSearchCV(estimator = rf_base, param_distributions = rf_grid,\n",
    "                               n_iter=2, cv = 5, verbose = 2,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_gscv.fit(X, y)\n",
    "\n",
    "# View the best parameters from the random search\n",
    "print(rf_gscv.best_params_)\n",
    "with open(\"rfr_cv_output.txt\",\"w\") as f:\n",
    "    f.write(rf_gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T15:06:31.108085Z",
     "start_time": "2022-06-01T15:06:31.097944Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "if scalar_on:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T15:17:21.508565Z",
     "start_time": "2022-06-01T15:06:31.109506Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma_range = list(np.logspace(-6, 3, 9))\n",
    "gamma_range.append(\"scale\")\n",
    "gamma_range.append(\"auto\")\n",
    "\n",
    "\n",
    "svr_grid = {\n",
    "#     \"estimator__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"estimator__kernel\": [\"rbf\"],\n",
    "    \"estimator__gamma\": [\"scale\", \"auto\"],\n",
    "    \"estimator__C\": [0.1, 1, 10, 100],\n",
    "    \"estimator__epsilon\": [0.001, 0.01, 0.1, 1, 10],\n",
    "#     \"estimator__shrinking\": [True, False]\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "svr_base = MultiOutputRegressor(SVR())\n",
    "svr_gscv = GridSearchCV(estimator = svr_base, param_grid = svr_grid,\n",
    "                        cv=3, verbose=2,\n",
    "                        n_jobs=-1)\n",
    "svr_gscv.fit(X, y)\n",
    "stop = time.time()\n",
    "print(svr_gscv.best_params_)\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define best parameters for kNN, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:36:30.977541Z",
     "start_time": "2022-09-21T09:36:30.975044Z"
    }
   },
   "outputs": [],
   "source": [
    "# For normal data\n",
    "# knn_best = {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}\n",
    "# svr_best = {'C': 1, 'epsilon': 0.01, 'gamma': 'auto', 'kernel': 'rbf'} # actual from GridSearchCV\n",
    "# svr_best = {'C': 0.01, 'epsilon': 0.01, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "\n",
    "# For CDT data\n",
    "knn_best = {'n_neighbors': 11, 'p': 2, 'weights': 'distance'}\n",
    "svr_best = {'C': 100, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models using k-fold cross-validation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/learning_curve.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T12:49:25.467041Z",
     "start_time": "2022-06-03T12:49:25.454455Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T12:49:29.847531Z",
     "start_time": "2022-06-03T12:49:29.841718Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_knn(**kwargs):\n",
    "    model = KNeighborsRegressor(**kwargs,\n",
    "                               n_jobs=-1)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_knn(X, y):\n",
    "    results_mae = []\n",
    "    results_mae_per = []\n",
    "    results_mre = []\n",
    "    results_mre_per = []\n",
    "    results_r2 = []\n",
    "    results_r2_per = []\n",
    "    \n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        if scalar_on:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        # define model\n",
    "        model = get_model_knn(**knn_best)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate model on test set\n",
    "        y_hat = model.predict(X_test)\n",
    "        \n",
    "        scores, scores_per = compute_scores(y_test, y_hat)\n",
    "        \n",
    "        # store result\n",
    "        print('>%.3f' % scores[0])\n",
    "        results_mae.append(scores[0])\n",
    "        results_mae_per.append(scores_per[0])\n",
    "        results_mre.append(scores[1])\n",
    "        results_mre_per.append(scores_per[1])\n",
    "        results_r2.append(scores[2])\n",
    "        results_r2_per.append(scores_per[2])\n",
    "    return results_mae, results_mae_per, results_mre, results_mre_per, results_r2, results_r2_per\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T12:49:38.594017Z",
     "start_time": "2022-06-03T12:49:31.362202Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_mre, results_mre_per, results_r2, results_r2_per = evaluate_model_knn(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (stop-start))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:35:34.134144Z",
     "start_time": "2021-12-09T14:35:34.131666Z"
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T14:42:31.306513Z",
     "start_time": "2021-09-21T14:42:31.297762Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model using best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T15:54:24.273783Z",
     "start_time": "2022-06-01T15:54:24.268382Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_rfr(**kwargs):\n",
    "    model = RandomForestRegressor(**kwargs)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_rfr(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        if scalar_on:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        # define model\n",
    "        model = get_model_rfr(**rf_gscv.best_params_)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate model on test set\n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test,y_hat,multioutput='raw_values')\n",
    "        r2 = r2_score(y_test,y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T15:54:25.118530Z",
     "start_time": "2022-06-01T15:54:25.114259Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per = evaluate_model_rfr(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T10:22:21.716186Z",
     "start_time": "2022-06-02T10:22:21.707788Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T10:23:17.140924Z",
     "start_time": "2022-06-02T10:23:17.134738Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_svr(**kwargs):\n",
    "    svr = SVR(**kwargs)\n",
    "\n",
    "    model = MultiOutputRegressor(svr)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_svr(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    svr_train_error = list()\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        if scalar_on:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        # define model\n",
    "        model = get_model_svr(**svr_best)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate model on test set\n",
    "        y_hat = model.predict(X_test)\n",
    "        scores, scores_per = compute_scores(y_test, y_hat)\n",
    "        svr_train_error.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "        # store result\n",
    "        print('>%.3f' % scores[0])\n",
    "        results_mae.append(scores[0])\n",
    "        results_mae_per.append(scores_per[0])\n",
    "        results_r2.append(scores[2])\n",
    "        results_r2_per.append(scores_per[2])\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, svr_train_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T10:23:58.797306Z",
     "start_time": "2022-06-02T10:23:17.757605Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, svr_train_error = evaluate_model_svr(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T13:39:33.097967Z",
     "start_time": "2022-05-05T13:39:33.094300Z"
    }
   },
   "outputs": [],
   "source": [
    "svr_validate = np.copy(results_mae)\n",
    "results_svr = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_svr[i,0] = svr_train_error[i]\n",
    "    results_svr[i,1] = svr_validate[i]\n",
    "    results_svr[i,2] = svr_validate[i]/svr_train_error[i]\n",
    "    \n",
    "np.savetxt(\"train_test_loss_svr.csv\", results_svr, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T11:28:15.072423Z",
     "start_time": "2022-06-27T11:28:15.064193Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:36:48.262040Z",
     "start_time": "2022-09-21T09:36:48.252200Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "weight_reg = 0.001\n",
    "lr = 0.001\n",
    "loss_metric = \"mae\"\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n",
    "    \n",
    "def get_model_mlp1(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=n_inputs,\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    activation=activations.swish,\n",
    "                    kernel_regularizer=l2(weight_reg), bias_regularizer=l2(weight_reg)\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_outputs, activation=activations.tanh))\n",
    "    model.compile(loss='mae', optimizer=Adam(learning_rate=lr), metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "def get_model_mlp3(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=n_inputs,\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    activation=activations.swish,\n",
    "                    kernel_regularizer=l2(weight_reg), bias_regularizer=l2(weight_reg)\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(100, activation=activations.swish,\n",
    "                    kernel_regularizer=l2(weight_reg), bias_regularizer=l2(weight_reg)\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(100, activation=activations.swish,\n",
    "                    kernel_regularizer=l2(weight_reg), bias_regularizer=l2(weight_reg)\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_outputs, activation=activations.tanh))\n",
    "    model.compile(loss='mae', optimizer=Adam(learning_rate=lr), metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model_mlp(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    mlp_trained = list()\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        if scalar_on:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        model = get_model_mlp1_test(n_inputs, n_outputs) # <<<<< SPECIFY WHICH MLP MODEL\n",
    "        \n",
    "        mlp_trained.append(model.fit(X_train, y_train, validation_split = 0.1, shuffle = False, epochs=200,\n",
    "                                     batch_size=32,\n",
    "                                     verbose=verbosity,\n",
    "#                                      callbacks=[callback]\n",
    "                                     )\n",
    "                          )\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        scores, scores_per = compute_scores(y_test, y_hat)\n",
    "        # store result\n",
    "        print('>%.3f' % scores[0])\n",
    "        results_mae.append(scores[0])\n",
    "        results_mae_per.append(scores_per[0])\n",
    "        results_r2.append(scores[2])\n",
    "        results_r2_per.append(scores_per[2])\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, mlp_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T12:54:10.971314Z",
     "start_time": "2022-06-03T12:50:02.945539Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, mlp_trained = evaluate_model_mlp(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T16:28:35.030098Z",
     "start_time": "2022-05-18T16:28:35.027674Z"
    }
   },
   "outputs": [],
   "source": [
    "name = \"mlp_1layer100n_dropout0_regular0_batch32_cdt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T16:28:36.437810Z",
     "start_time": "2022-05-18T16:28:36.431016Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp_validate = np.copy(results_mae)\n",
    "results_mlp = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_mlp[i,0] = mlp_trained[i].history[\"mae\"][-1]\n",
    "    results_mlp[i,1] = mlp_validate[i]\n",
    "    results_mlp[i,2] = mlp_validate[i]/mlp_trained[i].history[\"mae\"][-1]\n",
    "    \n",
    "np.savetxt(f\"overfit_test/history_{name}.csv\", results_mlp, delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T16:28:38.989571Z",
     "start_time": "2022-05-18T16:28:37.322047Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,figsize=(14,8), sharey = True, sharex = True)\n",
    "axs = axs.T.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(mlp_trained[i].history[\"loss\"])\n",
    "    ax.plot(mlp_trained[i].history[\"val_loss\"])\n",
    "    ax.set_xticks([0,100,200])\n",
    "    if i in [0,3,6,9,12,15]:\n",
    "        ax.set_title(f\"Fold {i//3+1:d}\")\n",
    "        \n",
    "axs[-1].legend([\"Train\",\"Validation\"], prop={'size': 12})\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('MAE')\n",
    "\n",
    "plt.savefig(f\"overfit_test/learning_curve_{name}.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T13:03:08.891974Z",
     "start_time": "2022-06-28T13:03:08.884242Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.copy(trace_train)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:36:55.049420Z",
     "start_time": "2022-09-21T09:36:55.035712Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "weight_reg = 0.001\n",
    "lr = 0.001\n",
    "conv2d_on = True\n",
    "\n",
    "def get_model_cnn3(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=6, input_shape=(200,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.swish))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(filters=32, kernel_size=6, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.swish))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=6, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activations.swish))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=activations.swish))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_outputs, activation=activations.tanh))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "def get_model_cnn1(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=6, input_shape=n_inputs))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=activations.swish))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_outputs, activation=activations.tanh))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_cnn1_2d(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32,\n",
    "                     kernel_size=2,\n",
    "                     input_shape=(200, 2, 1),\n",
    "                     kernel_regularizer=l2(weight_reg),\n",
    "                     # bias_regularizer=l2(weight_reg),\n",
    "                    )\n",
    "             )\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=activations.swish,\n",
    "                    kernel_regularizer=l2(weight_reg),\n",
    "                    # bias_regularizer=l2(weight_reg)\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_outputs, activation=activations.tanh))\n",
    "    model.compile(loss='mae', optimizer=Adam(learning_rate=lr), metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model_cnn(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    cnn_trained = list()\n",
    "    n_inputs, n_outputs = X.shape[1:], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        \n",
    "        if scalar_on:\n",
    "            scalers = {}\n",
    "            for i in range(X_train.shape[2]):\n",
    "                scalers[i] = StandardScaler()\n",
    "                X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "            for i in range(X_test.shape[2]):\n",
    "                X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "        if conv2d_on:\n",
    "            X_train = np.expand_dims(X_train, -1)\n",
    "            X_test = np.expand_dims(X_test, -1)\n",
    "        model = get_model_cnn1_2d(n_inputs, n_outputs)\n",
    "        \n",
    "        cnn_trained.append(model.fit(X_train, y_train, validation_split = 0.1, shuffle = False, epochs=200,\n",
    "                                      verbose=verbosity\n",
    "                                     )\n",
    "                          )\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        scores, scores_per = compute_scores(y_test, y_hat)\n",
    "        # store result\n",
    "        print('>%.3f' % scores[0])\n",
    "        results_mae.append(scores[0])\n",
    "        results_mae_per.append(scores_per[0])\n",
    "        results_r2.append(scores[2])\n",
    "        results_r2_per.append(scores_per[2])\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, cnn_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-28T13:21:16.255Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, cnn_trained = evaluate_model_cnn(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T17:23:11.017825Z",
     "start_time": "2022-06-27T17:23:11.015165Z"
    }
   },
   "outputs": [],
   "source": [
    "name = \"cnn1-2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T17:23:15.204467Z",
     "start_time": "2022-06-27T17:23:15.200748Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_validate = np.copy(results_mae)\n",
    "results_cnn = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_cnn[i,0] = cnn_trained[i].history[\"loss\"][-1]\n",
    "    results_cnn[i,1] = cnn_validate[i]\n",
    "    results_cnn[i,2] = cnn_validate[i]/cnn_trained[i].history[\"loss\"][-1]\n",
    "    \n",
    "# np.savetxt(f\"overfit_test/history_{name}.csv\", results_cnn, delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T17:23:21.340209Z",
     "start_time": "2022-06-27T17:23:19.622038Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,figsize=(14,8), sharey = True, sharex = True)\n",
    "axs = axs.T.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(cnn_trained[i].history[\"loss\"])\n",
    "    ax.plot(cnn_trained[i].history[\"val_loss\"])\n",
    "    ax.set_xticks([0,100,200])\n",
    "    if i in [0,3,6,9,12,15]:\n",
    "        ax.set_title(f\"Fold {i//3+1:d}\")\n",
    "        \n",
    "axs[-1].legend([\"Train\",\"Validation\"], prop={'size': 12})\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('MAE')\n",
    "\n",
    "plt.savefig(f\"overfit_test/learning_curve_{name}.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Neural Networks (FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:18.842021Z",
     "start_time": "2022-01-07T12:28:18.833647Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.copy(trace_train)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T11:31:09.464656Z",
     "start_time": "2022-01-11T11:31:09.451065Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_fcn(n_inputs, n_outputs):\n",
    "    x = keras.layers.Input(n_inputs)\n",
    "    drop_out = Dropout(0.1)(x)\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=8, input_shape=n_inputs, padding='same')(x) # default filter 128\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activations.swish)(conv1)\n",
    "\n",
    "    drop_out = Dropout(0.1)(conv1)\n",
    "    conv2 = keras.layers.Conv1D(filters=128, kernel_size=5, padding='same')(conv1) # default filter 256\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation(activations.swish)(conv2)\n",
    "\n",
    "    drop_out = Dropout(0.1)(conv2)\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding='same')(conv2) # default filter 128\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation(activations.swish)(conv3)\n",
    "\n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "#     full = keras.layers.GlobalMaxPooling1D()(conv3)\n",
    "    out = keras.layers.Dense(n_outputs)(full)\n",
    "    model = keras.models.Model(inputs=x, outputs=out)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def get_model_fcn_2(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, input_shape=n_inputs, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=256, kernel_size=5, input_shape=n_inputs, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, input_shape=n_inputs, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "#     model.add(MaxPooling1D())\n",
    "#     model.add(AveragePooling1D())\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "    \n",
    "def evaluate_model_fcn(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    fcn_trained = list()\n",
    "    n_inputs, n_outputs = X.shape[1:], y.shape[1]\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        X_train, X_test = X[train_ix,:,:], X[test_ix,:,:]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        nbatch, n_time, n_channel  = X_train.shape[0], X_train.shape[1], X_train.shape[2]\n",
    "        if scalar_on:\n",
    "            scalers = {}\n",
    "            for i in range(X_train.shape[2]):\n",
    "                scalers[i] = StandardScaler()\n",
    "                X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "            for i in range(X_test.shape[2]):\n",
    "                X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "        model = get_model_fcn(n_inputs, n_outputs)\n",
    "        \n",
    "        fcn_trained.append(model.fit(X_train, y_train,\n",
    "                                     validation_split = 0.1,\n",
    "                                     shuffle = True,\n",
    "                                     epochs=300,\n",
    "                                     verbose=verbosity\n",
    "                                     )\n",
    "                          )\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        scores, scores_per = compute_scores(y_test, y_hat)\n",
    "        # store result\n",
    "        print('>%.3f' % scores[0])\n",
    "        results_mae.append(scores[0])\n",
    "        results_mae_per.append(scores_per[0])\n",
    "        results_r2.append(scores[2])\n",
    "        results_r2_per.append(scores_per[2])\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, fcn_trained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T13:44:36.087184Z",
     "start_time": "2022-01-07T13:18:29.997537Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, fcn_trained = evaluate_model_fcn(X, y)\n",
    "keras.backend.clear_session()\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:02:28.935269Z",
     "start_time": "2022-01-06T19:02:28.932150Z"
    }
   },
   "outputs": [],
   "source": [
    "fcn_validate = np.copy(results_mae)\n",
    "results_fcn = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_fcn[i,0] = fcn_trained[i].history[\"mae\"][-1]\n",
    "    results_fcn[i,1] = fcn_validate[i]\n",
    "    results_fcn[i,2] = fcn_validate[i]/fcn_trained[i].history[\"mae\"][-1]\n",
    "    \n",
    "np.savetxt(\"fcn_train_test_loss.csv\", results_fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T13:44:38.892960Z",
     "start_time": "2022-01-07T13:44:36.089769Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,figsize=(14,8), sharey = True, sharex = True)\n",
    "axs = axs.T.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(fcn_trained[i].history[\"loss\"])\n",
    "    ax.plot(fcn_trained[i].history[\"val_loss\"])\n",
    "    ax.set_xticks([0,100,200])\n",
    "    if i in [0,3,6,9,12,15]:\n",
    "        ax.set_title(f\"Fold {i//3+1:d}\")\n",
    "        \n",
    "axs[-1].legend([\"Train\",\"Validation\"], prop={'size': 12})\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('MAE')\n",
    "\n",
    "plt.savefig(\"learning_curve_fcn_300epoch.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final train and evaluation on target test cases\n",
    "\n",
    "Note: need to run \"Define model...\" cells above to define model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:37:23.562802Z",
     "start_time": "2022-09-21T09:37:23.551839Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_data_knn():\n",
    "    X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "    X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "    y_train = af_train\n",
    "    y_test = af_test\n",
    "\n",
    "    if scalar_on:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_data_knn():\n",
    "    X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "    X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "    y_train = af_train\n",
    "    y_test = af_test\n",
    "\n",
    "    if scalar_on:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_data_knn():\n",
    "    X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "    X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "    y_train = af_train\n",
    "    y_test = af_test\n",
    "\n",
    "    if scalar_on:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_data_mlp():\n",
    "    \n",
    "    X_train = np.concatenate((trace_train[:,:,0], trace_train[:,:,1]), axis=1)\n",
    "    X_test = np.concatenate((trace_test[:,:,0], trace_test[:,:,1]), axis=1)\n",
    "    y_train = af_train\n",
    "    y_test = af_test\n",
    "\n",
    "    if scalar_on:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Standard Scalar:\", scalar_on)\n",
    "    print(\"Training size:\", y_train.shape[0])\n",
    "    print(\"Drug training:\", drug_train_on)\n",
    "    print(\"Dropout:\", dropout)\n",
    "    print(\"Weight reg:\", weight_reg)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def prep_data_cnn():\n",
    "    X_train = np.copy(trace_train)\n",
    "    X_test = np.copy(trace_test)\n",
    "    y_train = af_train\n",
    "    y_test = af_test\n",
    "\n",
    "    if scalar_on:\n",
    "        scalers = {}\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers[i] = StandardScaler()\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "    if conv2d_on:\n",
    "        X_train = np.expand_dims(X_train, -1)\n",
    "        X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "    print(\"Standard Scalar:\", scalar_on)\n",
    "    print(\"Training size:\", y_train.shape[0])\n",
    "    print(\"Drug training:\", drug_train_on)\n",
    "    print(\"Dropout:\", dropout)\n",
    "    print(\"Weight reg:\", weight_reg)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T13:01:45.989055Z",
     "start_time": "2022-06-02T13:01:45.955273Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model_knn(**knn_best)\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test).astype(float)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T13:01:49.586306Z",
     "start_time": "2022-06-02T13:01:49.457593Z"
    }
   },
   "outputs": [],
   "source": [
    "y_drug_hat = np.zeros((4,50,5))\n",
    "\n",
    "for i in range(af_drug_test.shape[0]):\n",
    "    X_drug_test = np.concatenate((trace_drug_test[i,:,:,0],trace_drug_test[i,:,:,1]),axis=1)\n",
    "    if scalar_on:\n",
    "        X_drug_test = scaler.transform(X_drug_test)\n",
    "    y_drug_hat[i,:,:] = model.predict(X_drug_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T15:13:35.008411Z",
     "start_time": "2022-01-13T15:13:35.003453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control\n",
    "with h5py.File(\"results_knn.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T13:01:57.432513Z",
     "start_time": "2022-06-02T13:01:57.426895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control + drug\n",
    "with h5py.File(\"results_drug_knn_cdt.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_drug_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_rfr()\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T10:57:13.559922Z",
     "start_time": "2022-05-06T10:56:59.923139Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model_svr(**svr_best)\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-06T10:57:39.357145Z",
     "start_time": "2022-05-06T10:57:38.007418Z"
    }
   },
   "outputs": [],
   "source": [
    "y_drug_hat = np.zeros((4,50,5))\n",
    "\n",
    "for i in range(af_drug_test.shape[0]):\n",
    "    X_drug_test = np.concatenate((trace_drug_test[i,:,:,0],trace_drug_test[i,:,:,1]),axis=1)\n",
    "    if scalar_on:\n",
    "        X_drug_test = scaler.transform(X_drug_test)\n",
    "    y_drug_hat[i,:,:] = model.predict(X_drug_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T15:14:01.706739Z",
     "start_time": "2022-01-13T15:14:01.700721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control\n",
    "with h5py.File(\"results_svr.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T20:34:28.439200Z",
     "start_time": "2022-03-14T20:34:28.433737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control + drug\n",
    "with h5py.File(\"results_drug_svr.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_drug_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:40:18.814622Z",
     "start_time": "2022-09-21T09:40:05.807011Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data_mlp()\n",
    "n_inputs, n_outputs = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "model = get_model_mlp3(n_inputs, n_outputs)\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=False, verbose=1)\n",
    "callback = []\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train, epochs=100, verbose=verbosity, callbacks=[callback])\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:40:21.366796Z",
     "start_time": "2022-09-21T09:40:21.361764Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"results_mlp3_drug-train_mps-prelim2.csv\", y_hat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T09:03:53.623320Z",
     "start_time": "2022-06-10T09:03:53.449037Z"
    }
   },
   "outputs": [],
   "source": [
    "y_drug_test = af_drug_test\n",
    "y_drug_hat = np.zeros((4,50,5))\n",
    "experiment = [\"control\", \"Kr\", \"CaL\", \"Kr+CaL\"]\n",
    "\n",
    "for i in range(af_drug_test.shape[0]):\n",
    "    X_drug_test = np.concatenate((trace_drug_test[i,:,:,0], trace_drug_test[i,:,:,1]), axis=1)\n",
    "    if scalar_on:\n",
    "        X_drug_test = scaler.transform(X_drug_test)\n",
    "    y_drug_hat[i,:,:] = model.predict(X_drug_test)\n",
    "    print(experiment[i])\n",
    "    print(r2_score(y_drug_test[i], y_drug_hat[i], multioutput='raw_values'))\n",
    "    print(mean_absolute_error(y_drug_test[i], y_drug_hat[i], multioutput='raw_values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T09:04:04.106819Z",
     "start_time": "2022-06-10T09:04:04.101994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control\n",
    "with h5py.File(\"results_mlp3-drugtrain-03.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T09:04:07.078336Z",
     "start_time": "2022-06-10T09:04:07.073773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control + drug\n",
    "with h5py.File(\"results_drug_mlp3-drugtrain-03.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_drug_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:41:11.319747Z",
     "start_time": "2022-09-21T09:40:53.309680Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data_cnn()\n",
    "n_inputs, n_outputs = X_train.shape[1:], y_train.shape[1]\n",
    "\n",
    "model = get_model_cnn1_2d(n_inputs, n_outputs) # <<< CHOOSE THIS\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=False, verbose=1)\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train, epochs=100, verbose=verbosity, callbacks = [callback])\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T09:42:45.961802Z",
     "start_time": "2022-09-21T09:42:45.957244Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"results_cnn1-2d_drugtrain_mlp-prelim2.csv\", y_hat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T17:37:47.002088Z",
     "start_time": "2022-06-27T17:37:46.841312Z"
    }
   },
   "outputs": [],
   "source": [
    "y_drug_test = af_drug_test\n",
    "y_drug_hat = np.zeros((4,50,5))\n",
    "experiment = [\"control\", \"Kr\", \"CaL\", \"Kr+CaL\"]\n",
    "\n",
    "for i in range(af_drug_test.shape[0]):\n",
    "    X_drug_test = np.copy(trace_drug_test[i,:,:,:])\n",
    "    if scalar_on:\n",
    "        for j in range(X_test.shape[2]):\n",
    "            X_drug_test[:, :, j] = scalers[j].transform(X_drug_test[:, :, j])\n",
    "    if conv2d_on:\n",
    "        X_drug_test = np.expand_dims(X_drug_test, axis=-1)\n",
    "    y_drug_hat[i,:,:] = model.predict(X_drug_test)\n",
    "    print(experiment[i])\n",
    "    print(r2_score(y_drug_test[i], y_drug_hat[i], multioutput='raw_values'))\n",
    "    print(mean_absolute_error(y_drug_test[i], y_drug_hat[i], multioutput='raw_values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T17:47:28.683964Z",
     "start_time": "2022-06-27T17:47:28.676307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control\n",
    "with h5py.File(\"results_cnn1-2d-drugtrain.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-27T17:47:29.451762Z",
     "start_time": "2022-06-27T17:47:29.447142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Control + drug\n",
    "with h5py.File(\"results_drug_cnn1-2d-drugtrain.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_drug_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T13:29:48.647725Z",
     "start_time": "2022-01-11T13:29:48.615501Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = trace_train\n",
    "X_test = trace_test\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "if scalar_on:\n",
    "    scalers = {}\n",
    "    for i in range(X_train.shape[2]):\n",
    "        scalers[i] = StandardScaler()\n",
    "        X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "    for i in range(X_test.shape[2]):\n",
    "        X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T13:31:14.278219Z",
     "start_time": "2022-01-11T13:30:01.145171Z"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = X_train.shape[1:], y_train.shape[1]\n",
    "\n",
    "model = get_model_fcn(n_inputs, n_outputs)\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train, epochs=200, verbose=verbosity)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T13:32:46.825668Z",
     "start_time": "2022-01-11T13:32:46.819472Z"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"results_fcn.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"adjustment_factors\", data=y_hat, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of $y_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T15:29:52.043140Z",
     "start_time": "2022-01-24T15:29:52.038220Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.mean(af_train,axis=0)\n",
    "y_test = af_test\n",
    "\n",
    "mae_if_mean = mean_absolute_error(y_test, np.tile(y_train, (y_test.shape[0],1)), multioutput='raw_values')\n",
    "print(mae_if_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble prediction interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:32:51.863838Z",
     "start_time": "2022-07-04T11:32:51.857042Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# fit an ensemble of models\n",
    "def fit_ensemble(n_members, X_train, X_test, y_train, y_test, **kwargs_fit):\n",
    "    ensemble = list()\n",
    "    for i in tqdm(range(n_members)):\n",
    "        n_inputs, n_outputs = X_train.shape[1], y_train.shape[1]\n",
    "#         model = get_model_mlp3(n_inputs, n_outputs)\n",
    "        model = get_model_cnn1_2d(n_inputs, n_outputs)\n",
    "        model.fit(X_train, y_train, **kwargs_fit)\n",
    "        ensemble.append(model)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "# make predictions with the ensemble and calculate a prediction interval\n",
    "def predict_with_pi(ensemble, X_i):\n",
    "    # make predictions\n",
    "    y_hat_i = np.asarray([model.predict(X_i, verbose=0) for model in ensemble])[:,0,:]\n",
    "    # mean value across ensemble\n",
    "    mean_y_hat_i = np.mean(y_hat_i, axis=0)\n",
    "    interval = np.std(y_hat_i, axis=0)\n",
    "    lower_i = mean_y_hat_i - interval\n",
    "    upper_i = mean_y_hat_i + interval\n",
    "    return y_hat_i, mean_y_hat_i, lower_i, upper_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-04T11:33:55.926Z"
    }
   },
   "outputs": [],
   "source": [
    "n_members = 500\n",
    "X_train, X_test, y_train, y_test = prep_data_cnn()\n",
    "ensemble = fit_ensemble(n_members, X_train, X_test, y_train, y_test, verbose=verbosity, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on test set with prediction interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-04T11:34:00.490Z"
    }
   },
   "outputs": [],
   "source": [
    "saveit=1\n",
    "\n",
    "# Compute prediction interval for each point in y_test\n",
    "y_hat_all = np.zeros((50, 500, 5))\n",
    "y_lower = np.zeros_like(y_test)\n",
    "y_upper = np.zeros_like(y_test)\n",
    "y_mean = np.zeros_like(y_test)\n",
    "for i in tqdm(range(y_test.shape[0])):\n",
    "    X_i = np.expand_dims(X_test[i, :], axis=0)\n",
    "    y_hat_all[i,:,:], y_mean[i,:], y_lower[i,:],  y_upper[i,:] = predict_with_pi(ensemble, X_i)\n",
    "    \n",
    "if saveit:\n",
    "    np.save(\"results_pi_cnn1-2d-02_ne500.npy\", y_hat_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediction with interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T12:56:41.786683Z",
     "start_time": "2022-06-28T12:56:41.000247Z"
    }
   },
   "outputs": [],
   "source": [
    "saveit = 1\n",
    "plt.figure(1, figsize=(10,8))\n",
    "p_names = [\"Kr\", \"CaL\", \"K1\", \"Na\", \"NaL\"]\n",
    "p_idx = 4\n",
    "for i in range(X_test.shape[0]):\n",
    "    plt.plot(y_test[i, p_idx], y_mean[i, p_idx],'ko', alpha=0.6, markersize=4)\n",
    "    plt.plot([y_test[i, p_idx], y_test[i, p_idx]],[y_lower[i, p_idx], y_upper[i, p_idx]], 'r-', lw=0.6, zorder=0)\n",
    "# plt.xticks([-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8])\n",
    "# plt.yticks([-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8])\n",
    "plt.xlabel('$\\lambda_{true}$')\n",
    "plt.ylabel('$\\lambda_{estimate}$')\n",
    "plt.legend([\"Mean estimate\", \"Standard deviation\"])\n",
    "plt.grid()\n",
    "# plt.axis('equal')\n",
    "if saveit:\n",
    "    plt.savefig(f\"plotpi_cnn1-2d-01_ne500_{p_names[p_idx]}.png\",\n",
    "                dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "441px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
