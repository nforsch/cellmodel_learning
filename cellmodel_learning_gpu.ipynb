{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:27:55.282866Z",
     "start_time": "2022-01-07T12:27:52.452551Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, RepeatedKFold, cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalAveragePooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['savefig.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:27:59.116968Z",
     "start_time": "2022-01-07T12:27:55.293995Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'tf.test.is_built_with_cuda(): {tf.test.is_built_with_cuda()}');\n",
    "# print(f'tf.test.is_gpu_available(): {tf.test.is_gpu_available()}')\n",
    "\n",
    "if tf.__version__[0] == '1':\n",
    "    print(f'tf.config.experimental_list_devices(): {tf.config.experimental_list_devices()}');\n",
    "else:\n",
    "    print(f'tf.config.list_physical_devices(\"GPU\"): {tf.config.list_physical_devices(\"GPU\")}');\n",
    "    \n",
    "print(f'tf.test.gpu_device_name(): {tf.test.gpu_device_name()}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:03.013081Z",
     "start_time": "2022-01-07T12:28:03.009995Z"
    }
   },
   "outputs": [],
   "source": [
    "# randst = np.random.randint(0,100)\n",
    "randst = 94\n",
    "print(randst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:05.417092Z",
     "start_time": "2022-01-07T12:28:05.414693Z"
    }
   },
   "outputs": [],
   "source": [
    "verbosity = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:07.408828Z",
     "start_time": "2022-01-07T12:28:07.404681Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_my_results(results_mae, results_mae_per, results_r2, results_r2_per):\n",
    "    \n",
    "    print('MAE:')\n",
    "    print('\\t%.3f (%.3f) overall' % (np.mean(results_mae), np.std(results_mae)))\n",
    "    for mae,std,idx in zip(np.mean(results_mae_per, axis=0), np.std(results_mae_per, axis=0), idx_params):\n",
    "        print('\\t%.3f (%.3f) %s' % (mae, std, labels[idx]) )\n",
    "    print('R2:')\n",
    "    print('\\t%.3f (%.3f) overall' %(np.mean(results_r2), np.std(results_r2)))\n",
    "    for r2,std,idx in zip(np.mean(results_r2_per, axis=0), np.std(results_r2_per, axis=0), idx_params):\n",
    "        print('\\t%.3f (%.3f) %s' % (r2, std, labels[idx]) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and partition data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and test case indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:09.249429Z",
     "start_time": "2022-01-07T12:28:09.230683Z"
    }
   },
   "outputs": [],
   "source": [
    "name = \"n4130\"\n",
    "here = Path.cwd()\n",
    "data_path = Path(here.joinpath(f\"data/data_control_{name}.h5\"))\n",
    "\n",
    "with h5py.File(data_path, 'r') as f:\n",
    "    Datasetnames=f.keys()\n",
    "    print(*list(Datasetnames), sep = \"\\n\")\n",
    "    trace = f['trace'][:,:200,:] # select time 0-200\n",
    "    t = f['time'][...]\n",
    "    adj_factors = f['adjustment_factors'][...]\n",
    "    cost_terms = f['cost_terms'][...]\n",
    "    \n",
    "if trace.shape[0] != adj_factors.shape[0]:\n",
    "    print('Number of samples do not match for trace and adj_factors!')\n",
    "\n",
    "print(\"Number of traces:\", trace.shape[0])\n",
    "labels = [\"g_Kr\",\"g_CaL\",\"lambda_B\",\"g_NaCa\",\"g_K1\",\"J_SERCA_bar\",\"lambda_diff\",\"lambda_RyR\",\"g_bCa\",\"g_Na\",\"g_NaL\"]\n",
    "\n",
    "idx_all = list(np.arange(0,trace.shape[0]))\n",
    "idx_test = list(np.loadtxt(here.joinpath(\"data/idx_key_p11_s100_n5000_ns50.txt\"), dtype=int))\n",
    "idx_train = list(set(idx_all) - set(idx_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition data\n",
    "Test cases are pre-selected for out-of-sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:12.490556Z",
     "start_time": "2022-01-07T12:28:12.481784Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_params = [0,1,4,9,10]\n",
    "trace_train = trace[idx_train,:,:]\n",
    "trace_test = trace[idx_test,:,:]\n",
    "af_train = adj_factors[idx_train,:][:,idx_params]\n",
    "af_test = adj_factors[idx_test,:][:,idx_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning for kNN, RF, SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:52:50.637433Z",
     "start_time": "2022-01-05T13:52:50.607818Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T13:55:17.682671Z",
     "start_time": "2022-01-05T13:53:20.695327Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_grid = {\"n_neighbors\": np.arange(1, 20),\n",
    "            \"weights\": ['uniform', 'distance'],\n",
    "            \"p\": [1,2],\n",
    "              }\n",
    "\n",
    "knn_base = KNeighborsRegressor()\n",
    "knn_gscv = GridSearchCV(estimator = knn_base, param_grid = knn_grid,\n",
    "                        cv=5, verbose=2,\n",
    "                        n_jobs=-1)\n",
    "knn_gscv.fit(X, y)\n",
    "\n",
    "print(knn_gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T12:14:39.727171Z",
     "start_time": "2022-01-06T12:14:39.698549Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T13:46:23.191697Z",
     "start_time": "2022-01-06T12:56:13.369729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of trees in Random Forest\n",
    "rf_n_estimators = [int(x) for x in np.linspace(200, 1000, 5)]\n",
    "rf_n_estimators.append(1500)\n",
    "rf_n_estimators.append(2000)\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "rf_max_depth = [int(x) for x in np.linspace(5, 55, 6)]\n",
    "rf_max_depth.append(None)\n",
    "\n",
    "# Number of features to consider at every split\n",
    "# rf_max_features = ['auto', 'sqrt', 'log2']\n",
    "rf_max_features = ['log2']\n",
    "\n",
    "# Criterion to split on\n",
    "rf_criterion = ['absolute_error']\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "rf_min_samples_split = [int(x) for x in np.linspace(2, 10, 9)]\n",
    "\n",
    "# Minimum decrease in impurity required for split to happen\n",
    "rf_min_impurity_decrease = [0.0, 0.05, 0.1]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "rf_bootstrap = [True, False]\n",
    "\n",
    "# Create the grid\n",
    "rf_grid = {'n_estimators': rf_n_estimators,\n",
    "               'max_depth': rf_max_depth,\n",
    "               'max_features': rf_max_features,\n",
    "               'criterion': rf_criterion,\n",
    "               'min_samples_split': rf_min_samples_split,\n",
    "               'min_impurity_decrease': rf_min_impurity_decrease,\n",
    "               'bootstrap': rf_bootstrap}\n",
    "\n",
    "rf_base = RandomForestRegressor()\n",
    "\n",
    "# Create the random search Random Forest\n",
    "rf_gscv = RandomizedSearchCV(estimator = rf_base, param_distributions = rf_grid,\n",
    "                               n_iter=2, cv = 5, verbose = 2,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_gscv.fit(X, y)\n",
    "\n",
    "# View the best parameters from the random search\n",
    "print(rf_gscv.best_params_)\n",
    "with open(\"rfr_cv_output.txt\",\"w\") as f:\n",
    "    f.write(rf_gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T20:17:47.320155Z",
     "start_time": "2022-01-05T20:17:43.365650Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-05T20:18:11.961Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma_range = list(np.logspace(-6, 3, 9))\n",
    "gamma_range.append(\"scale\")\n",
    "gamma_range.append(\"auto\")\n",
    "\n",
    "\n",
    "svr_grid = {\n",
    "#     \"estimator__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"estimator__kernel\": [\"rbf\"],\n",
    "    \"estimator__gamma\": [\"scale\", \"auto\"],\n",
    "    \"estimator__C\": [1, 10, 100, 1000],\n",
    "    \"estimator__epsilon\": [0.001, 0.01, 0.1, 1, 10],\n",
    "#     \"estimator__shrinking\": [True, False]\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "svr_base = MultiOutputRegressor(SVR())\n",
    "svr_gscv = GridSearchCV(estimator = svr_base, param_grid = svr_grid,\n",
    "                        cv=3, verbose=2,\n",
    "                        n_jobs=-1)\n",
    "svr_gscv.fit(X, y)\n",
    "stop = time.time()\n",
    "print(svr_gscv.best_params_)\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define best parameters for kNN, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:36:41.326856Z",
     "start_time": "2022-01-07T10:36:41.324376Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_best = {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}\n",
    "\n",
    "svr_best = {'C': 1, 'epsilon': 0.01, 'gamma': 'auto', 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models using k-fold cross-validation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/learning_curve.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:40:19.149885Z",
     "start_time": "2022-01-07T10:40:19.139794Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:40:20.613241Z",
     "start_time": "2022-01-07T10:40:20.607767Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_knn(**kwargs):\n",
    "    model = KNeighborsRegressor(**kwargs,\n",
    "                               n_jobs=-1)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_knn(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    knn_train_error = list()\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # define model\n",
    "        model = get_model_knn(**knn_best)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate model on test set\n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test, y_hat, multioutput='raw_values')\n",
    "        r2 = r2_score(y_test, y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        knn_train_error.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, knn_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:40:46.532200Z",
     "start_time": "2022-01-07T10:40:37.680740Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, knn_trained_error = evaluate_model_knn(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (stop-start))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training to validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_knn = np.zeros(5)\n",
    "for i in range(5):\n",
    "    ratio_knn[i] = results_knn[i]/knn_trained_error[i].history[\"mae\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T14:35:34.134144Z",
     "start_time": "2021-12-09T14:35:34.131666Z"
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T14:42:31.306513Z",
     "start_time": "2021-09-21T14:42:31.297762Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model using best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T14:42:33.422909Z",
     "start_time": "2021-09-21T14:42:33.415618Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_rfr(**kwargs):\n",
    "    model = RandomForestRegressor(**kwargs)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_rfr(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # define model\n",
    "        model = get_model_rfr(**rf_gscv.best_params_)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate model on test set\n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test,y_hat,multioutput='raw_values')\n",
    "        r2 = r2_score(y_test,y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T14:54:38.001053Z",
     "start_time": "2021-09-21T14:42:37.900687Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per = evaluate_model_rfr(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:43:08.249932Z",
     "start_time": "2022-01-07T10:43:08.239688Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:43:22.688416Z",
     "start_time": "2022-01-07T10:43:22.682999Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_svr(**kwargs):\n",
    "    svr = SVR(**kwargs)\n",
    "\n",
    "    model = MultiOutputRegressor(svr)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_svr(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    svr_train_error = list()\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        # prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # define model\n",
    "        model = get_model_svr(**svr_best)\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate model on test set\n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test,y_hat,multioutput='raw_values')\n",
    "        r2 = r2_score(y_test,y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        svr_train_error.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "        # store result\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, svr_train_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:49:34.198977Z",
     "start_time": "2022-01-07T10:43:32.292770Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, svr_train_error = evaluate_model_svr(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:54:29.952460Z",
     "start_time": "2022-01-07T10:54:29.946909Z"
    }
   },
   "outputs": [],
   "source": [
    "svr_validate = np.copy(results_mae)\n",
    "results_svr = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_svr[i,0] = svr_train_error[i]\n",
    "    results_svr[i,1] = svr_validate[i]\n",
    "    results_svr[i,2] = svr_validate[i]/svr_train_error[i]\n",
    "    \n",
    "np.savetxt(\"svr_train_test_loss.csv\", results_svr, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T10:58:20.339733Z",
     "start_time": "2022-01-07T10:58:20.330593Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T11:03:18.462630Z",
     "start_time": "2022-01-07T11:03:18.454537Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_mlp1(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=n_inputs, kernel_initializer='he_uniform', activation=activations.swish))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "def get_model_mlp3(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=n_inputs, kernel_initializer='he_uniform', activation=activations.swish))\n",
    "    model.add(Dense(500, activation=activations.swish))\n",
    "    model.add(Dense(500, activation=activations.swish))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model_mlp(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    mlp_trained = list()\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        model = get_model_mlp3(n_inputs, n_outputs) # <<<<< SPECIFY WHICH MLP MODEL\n",
    "        \n",
    "        mlp_trained.append(model.fit(X_train, y_train, validation_split = 0.1, shuffle = False, epochs=200,\n",
    "                                      verbose=verbosity\n",
    "                                     )\n",
    "                          )\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test,y_hat,multioutput='raw_values')\n",
    "        r2 = r2_score(y_test,y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, mlp_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T11:12:47.418196Z",
     "start_time": "2022-01-07T11:03:19.427337Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, mlp_trained = evaluate_model_mlp(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:13:34.491447Z",
     "start_time": "2022-01-06T19:13:34.384223Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp_validate = np.copy(results_mae)\n",
    "results_mlp = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_mlp[i,0] = mlp_trained[i].history[\"mae\"][-1]\n",
    "    results_mlp[i,1] = mlp_validate[i]\n",
    "    results_mlp[i,2] = mlp_validate[i]/mlp_trained[i].history[\"mae\"][-1]\n",
    "    \n",
    "np.savetxt(\"mlp_train_test_loss.csv\", results_mlp, delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:18:59.807182Z",
     "start_time": "2022-01-07T12:18:57.941371Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,figsize=(14,8), sharey = True, sharex = True)\n",
    "axs = axs.T.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(mlp_trained[i].history[\"loss\"])\n",
    "    ax.plot(mlp_trained[i].history[\"val_loss\"])\n",
    "    ax.set_xticks([0,100,200])\n",
    "    if i in [0,3,6,9,12,15]:\n",
    "        ax.set_title(f\"Fold {i//3+1:d}\")\n",
    "        \n",
    "axs[-1].legend([\"Train\",\"Validation\"], prop={'size': 12})\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('MAE')\n",
    "\n",
    "plt.savefig(\"learning_curve_mlp.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T11:33:12.489994Z",
     "start_time": "2022-01-07T11:33:12.475050Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.copy(trace_train)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:08:00.910785Z",
     "start_time": "2022-01-07T12:08:00.902488Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_cnn(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, input_shape=n_inputs))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation=activations.swish))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=\"mae\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model_cnn(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    cnn_trained = list()\n",
    "    n_inputs, n_outputs = X.shape[1:], y.shape[1]\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    # enumerate folds\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        scalers = {}\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers[i] = StandardScaler()\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "        model = get_model_cnn(n_inputs, n_outputs)\n",
    "        \n",
    "        cnn_trained.append(model.fit(X_train, y_train, validation_split = 0.1, shuffle = False, epochs=200,\n",
    "                                      verbose=verbosity\n",
    "                                     )\n",
    "                          )\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test,y_hat,multioutput='raw_values')\n",
    "        r2 = r2_score(y_test,y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, cnn_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:17:41.908963Z",
     "start_time": "2022-01-07T12:08:02.358460Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, cnn_trained = evaluate_model_cnn(X, y)\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:31:12.599565Z",
     "start_time": "2022-01-06T19:31:12.489420Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_validate = np.copy(results_mae)\n",
    "results_cnn = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_cnn[i,0] = cnn_trained[i].history[\"loss\"][-1]\n",
    "    results_cnn[i,1] = cnn_validate[i]\n",
    "    results_cnn[i,2] = cnn_validate[i]/cnn_trained[i].history[\"loss\"][-1]\n",
    "    \n",
    "np.savetxt(\"cnn_train_test_loss.csv\", results_cnn, delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:18:42.209844Z",
     "start_time": "2022-01-07T12:18:40.432240Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,figsize=(14,8), sharey = True, sharex = True)\n",
    "axs = axs.T.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(cnn_trained[i].history[\"loss\"])\n",
    "    ax.plot(cnn_trained[i].history[\"val_loss\"])\n",
    "    ax.set_xticks([0,100,200])\n",
    "    if i in [0,3,6,9,12,15]:\n",
    "        ax.set_title(f\"Fold {i//3+1:d}\")\n",
    "        \n",
    "axs[-1].legend([\"Train\",\"Validation\"], prop={'size': 12})\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('MAE')\n",
    "\n",
    "plt.savefig(\"learning_curve_cnn.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Neural Networks (FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T12:28:18.842021Z",
     "start_time": "2022-01-07T12:28:18.833647Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.copy(trace_train)\n",
    "y = af_train\n",
    "\n",
    "print('X shape:',X.shape)\n",
    "print('Feature shape:',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T13:18:28.784753Z",
     "start_time": "2022-01-07T13:18:28.771193Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_fcn(n_inputs, n_outputs):\n",
    "    x = keras.layers.Input(n_inputs)\n",
    "    drop_out = Dropout(0.1)(x)\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=8, input_shape=n_inputs, padding='same')(x) # default filter 128\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activations.swish)(conv1)\n",
    "\n",
    "    drop_out = Dropout(0.1)(conv1)\n",
    "    conv2 = keras.layers.Conv1D(filters=128, kernel_size=5, padding='same')(conv1) # default filter 256\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation(activations.swish)(conv2)\n",
    "\n",
    "    drop_out = Dropout(0.1)(conv2)\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding='same')(conv2) # default filter 128\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation(activations.swish)(conv3)\n",
    "\n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "#     full = keras.layers.GlobalMaxPooling1D()(conv3)\n",
    "    out = keras.layers.Dense(n_outputs)(full)\n",
    "    model = keras.models.Model(inputs=x, outputs=out)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def get_model_fcn_2(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=128, kernel_size=8, input_shape=n_inputs, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=256, kernel_size=5, input_shape=n_inputs, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, input_shape=n_inputs, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "#     model.add(MaxPooling1D())\n",
    "#     model.add(AveragePooling1D())\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "    \n",
    "def evaluate_model_fcn(X, y):\n",
    "    results_mae = list()\n",
    "    results_mae_per = list()\n",
    "    results_r2 = list()\n",
    "    results_r2_per = list()\n",
    "    fcn_trained = list()\n",
    "    n_inputs, n_outputs = X.shape[1:], y.shape[1]\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=randst)\n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        X_train, X_test = X[train_ix,:,:], X[test_ix,:,:]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        nbatch, n_time, n_channel  = X_train.shape[0], X_train.shape[1], X_train.shape[2]\n",
    "        scalers = {}\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers[i] = StandardScaler()\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "        model = get_model_fcn(n_inputs, n_outputs)\n",
    "        \n",
    "        fcn_trained.append(model.fit(X_train, y_train,\n",
    "                                     validation_split = 0.1,\n",
    "                                     shuffle = True,\n",
    "                                     epochs=300,\n",
    "                                     verbose=verbosity\n",
    "                                     )\n",
    "                          )\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        r2_per = r2_score(y_test,y_hat,multioutput='raw_values')\n",
    "        r2 = r2_score(y_test,y_hat)\n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mae_per = mean_absolute_error(y_test, y_hat, multioutput='raw_values')\n",
    "        print('>%.3f' % mae)\n",
    "        results_mae.append(mae)\n",
    "        results_mae_per.append(mae_per)\n",
    "        results_r2.append(r2)\n",
    "        results_r2_per.append(r2_per)\n",
    "    return results_mae, results_mae_per, results_r2, results_r2_per, fcn_trained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T13:44:36.087184Z",
     "start_time": "2022-01-07T13:18:29.997537Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results_mae, results_mae_per, results_r2, results_r2_per, fcn_trained = evaluate_model_fcn(X, y)\n",
    "keras.backend.clear_session()\n",
    "stop = time.time()\n",
    "print('Time of execution: %f' % (np.divide(stop-start, 60)))\n",
    "\n",
    "print_my_results(results_mae, results_mae_per, results_r2, results_r2_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ratio of training and validation error (last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:02:28.935269Z",
     "start_time": "2022-01-06T19:02:28.932150Z"
    }
   },
   "outputs": [],
   "source": [
    "fcn_validate = np.copy(results_mae)\n",
    "results_fcn = np.zeros((15,3))\n",
    "for i in range(15):\n",
    "    results_fcn[i,0] = fcn_trained[i].history[\"mae\"][-1]\n",
    "    results_fcn[i,1] = fcn_validate[i]\n",
    "    results_fcn[i,2] = fcn_validate[i]/fcn_trained[i].history[\"mae\"][-1]\n",
    "    \n",
    "np.savetxt(\"fcn_train_test_loss.csv\", results_fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T13:44:38.892960Z",
     "start_time": "2022-01-07T13:44:36.089769Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,5,figsize=(14,8), sharey = True, sharex = True)\n",
    "axs = axs.T.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(fcn_trained[i].history[\"loss\"])\n",
    "    ax.plot(fcn_trained[i].history[\"val_loss\"])\n",
    "    ax.set_xticks([0,100,200])\n",
    "    if i in [0,3,6,9,12,15]:\n",
    "        ax.set_title(f\"Fold {i//3+1:d}\")\n",
    "        \n",
    "axs[-1].legend([\"Train\",\"Validation\"], prop={'size': 12})\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('MAE')\n",
    "\n",
    "plt.savefig(\"learning_curve_fcn_300epoch.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final train and evaluation on target test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transformformsform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_knn()\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_rfr()\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_svr()\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((trace_train[:,:,0],trace_train[:,:,1]),axis=1)\n",
    "X_test = np.concatenate((trace_test[:,:,0],trace_test[:,:,1]),axis=1)\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "model = get_model_mlp1(n_inputs, n_outputs)\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trace_train\n",
    "X_test = trace_test\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X_train.shape[2]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = X_train.shape[1:], y_train.shape[1]\n",
    "\n",
    "model = get_model_cnn(n_inputs, n_outputs)\n",
    "\n",
    "t_train_start = time.time()\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "t_train_stop = time.time()\n",
    "\n",
    "t_test_start = time.time()\n",
    "y_hat = model.predict(X_test)\n",
    "t_test_stop = time.time()\n",
    "\n",
    "print(\"Time to train model: \", t_train_stop-t_train_start)\n",
    "print(\"Time to test model: \", t_test_stop-t_test_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trace_train\n",
    "X_test = trace_test\n",
    "y_train = af_train\n",
    "y_test = af_test\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X_train.shape[2]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs, n_outputs = X_train.shape[1:], y_train.shape[1]\n",
    "\n",
    "model = get_model_fcn(n_inputs, n_outputs)\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "436px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
